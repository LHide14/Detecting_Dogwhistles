{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:17:56.762237200Z",
     "start_time": "2023-08-24T10:17:55.544514200Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_lem.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:17:56.804161300Z",
     "start_time": "2023-08-24T10:17:56.767224500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "                                     lemmatized  target\n0                              ['hate', 'thug']       1\n1  ['really', \"can't\", 'stand', 'thug', 'like']       1\n2                           ['despise', 'thug']       1\n3                            ['detest', 'thug']       1\n4              ['absolutely', 'loathe', 'thug']       1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lemmatized</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>['hate', 'thug']</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>['really', \"can't\", 'stand', 'thug', 'like']</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>['despise', 'thug']</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>['detest', 'thug']</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>['absolutely', 'loathe', 'thug']</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:17:56.821112800Z",
     "start_time": "2023-08-24T10:17:56.800138200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import ast ## This module can be used to evaluate literals, eg: transform string-lists back into lists\n",
    "def extract_genres(x):\n",
    "    x = ast.literal_eval(x)\n",
    "    return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:17:56.826066600Z",
     "start_time": "2023-08-24T10:17:56.813101300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "                           lemmatized  target\n0                        [hate, thug]       1\n1  [really, can't, stand, thug, like]       1\n2                     [despise, thug]       1\n3                      [detest, thug]       1\n4          [absolutely, loathe, thug]       1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lemmatized</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[hate, thug]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[really, can't, stand, thug, like]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[despise, thug]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[detest, thug]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[absolutely, loathe, thug]</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lemmatized'] = df['lemmatized'].apply(extract_genres)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:17:56.999607200Z",
     "start_time": "2023-08-24T10:17:56.841027Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "                     lemmatized  target\n0                     hate thug       1\n1  really can't stand thug like       1\n2                  despise thug       1\n3                   detest thug       1\n4        absolutely loathe thug       1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lemmatized</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>hate thug</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>really can't stand thug like</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>despise thug</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>detest thug</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>absolutely loathe thug</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lemmatized'] = [' '.join(x) for x in df['lemmatized']]\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:17:57.048472900Z",
     "start_time": "2023-08-24T10:17:57.001597800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "X = df['lemmatized']\n",
    "\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=14)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:17:57.133246Z",
     "start_time": "2023-08-24T10:17:57.033514200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "                                             lemmatized  target\n1839                        nothing common ghetto trahs       1\n7092  never support politician treat autoandrophiles...       0\n9024              I'd rather cancre anywhere near Aiden       1\n5473                                   banker c n c e r       1\n9764                            Saying make bit bastard       0\n...                                                 ...     ...\n7526                               clownfishs beautiful       0\n6471                           hurt autogynephiles like       1\n2454                                   bings belong zoo       1\n9484                     obvious troons opposite stupid       0\n2667                           bings like absolutefilth       1\n\n[7983 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lemmatized</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1839</th>\n      <td>nothing common ghetto trahs</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7092</th>\n      <td>never support politician treat autoandrophiles...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9024</th>\n      <td>I'd rather cancre anywhere near Aiden</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5473</th>\n      <td>banker c n c e r</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9764</th>\n      <td>Saying make bit bastard</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7526</th>\n      <td>clownfishs beautiful</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6471</th>\n      <td>hurt autogynephiles like</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2454</th>\n      <td>bings belong zoo</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9484</th>\n      <td>obvious troons opposite stupid</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2667</th>\n      <td>bings like absolutefilth</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>7983 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xy_train = pd.concat([X_train, y_train], axis=1)\n",
    "Xy_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:17:57.163166800Z",
     "start_time": "2023-08-24T10:17:57.045481Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "1    5833\n0    2150\nName: target, dtype: int64"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xy_train.target.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:17:57.237968300Z",
     "start_time": "2023-08-24T10:17:57.060441200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Upsampling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "majority = Xy_train[Xy_train['target'] == 1]\n",
    "minority = Xy_train[Xy_train['target'] == 0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:17:57.269881200Z",
     "start_time": "2023-08-24T10:17:57.078393Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "1    5833\n0    5833\nName: target, dtype: int64"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minority_upsampled = resample(minority, replace=True, n_samples=5833, random_state=14)\n",
    "upsampled = pd.concat([majority, minority_upsampled])\n",
    "upsampled['target'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:17:57.296808500Z",
     "start_time": "2023-08-24T10:17:57.094351Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "X_train_up = upsampled['lemmatized']\n",
    "y_train_up = upsampled['target']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:17:57.297805800Z",
     "start_time": "2023-08-24T10:17:57.107315800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# vectorizer = CountVectorizer()\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                             lowercase=True,\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = None,\n",
    "                             max_features = 100)\n",
    "X_train_up_vec = vectorizer.fit_transform(X_train_up)\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:17:57.332715500Z",
     "start_time": "2023-08-24T10:17:57.124269800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression()",
      "text/html": "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train_up_vec, y_train_up)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:17:57.494709600Z",
     "start_time": "2023-08-24T10:17:57.233977300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7764443682496143\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_train_up_vec)\n",
    "accuracy = accuracy_score(y_train_up, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:17:57.494709600Z",
     "start_time": "2023-08-24T10:17:57.332715500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7710420841683366\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test_vec)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:17:57.495707500Z",
     "start_time": "2023-08-24T10:17:57.339696800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Function to test max_features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "max_list = [200, 250, 300, 350, 400]\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "for x in max_list:\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                                 lowercase=True,\n",
    "                                 tokenizer = None,\n",
    "                                 preprocessor = None,\n",
    "                                 stop_words = None,\n",
    "                                 max_features = x)\n",
    "\n",
    "    X_train_up_vec = vectorizer.fit_transform(X_train_up)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "    classifier = LogisticRegression()\n",
    "    classifier.fit(X_train_up_vec, y_train_up)\n",
    "\n",
    "    y_pred_train = classifier.predict(X_train_up_vec)\n",
    "    acc = accuracy_score(y_train_up, y_pred_train)\n",
    "    train_accuracy.append(acc)\n",
    "\n",
    "    y_pred_test = classifier.predict(X_test_vec)\n",
    "    acc2 = accuracy_score(y_test, y_pred_test)\n",
    "    test_accuracy.append(acc2)\n",
    "\n",
    "data = {'max_features': max_list, 'train_accuracy': train_accuracy, 'test_accuracy': test_accuracy}\n",
    "\n",
    "max_df = pd.DataFrame(data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:17:58.332371100Z",
     "start_time": "2023-08-24T10:17:57.354654100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "   max_features  train_accuracy  test_accuracy\n0           200        0.849991       0.860721\n1           250        0.927910       0.923848\n2           300        0.970341       0.964429\n3           350        0.986628       0.983968\n4           400        0.989799       0.984970",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>max_features</th>\n      <th>train_accuracy</th>\n      <th>test_accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>200</td>\n      <td>0.849991</td>\n      <td>0.860721</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>250</td>\n      <td>0.927910</td>\n      <td>0.923848</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>300</td>\n      <td>0.970341</td>\n      <td>0.964429</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>350</td>\n      <td>0.986628</td>\n      <td>0.983968</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>400</td>\n      <td>0.989799</td>\n      <td>0.984970</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:17:58.348454700Z",
     "start_time": "2023-08-24T10:17:58.334366600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Burrowing Down Further"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "max_list = [300, 305, 310, 315, 320, 330, 340, 350]\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "for x in max_list:\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                                 lowercase=True,\n",
    "                                 tokenizer = None,\n",
    "                                 preprocessor = None,\n",
    "                                 stop_words = None,\n",
    "                                 max_features = x)\n",
    "\n",
    "    X_train_up_vec = vectorizer.fit_transform(X_train_up)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "    classifier = LogisticRegression()\n",
    "    classifier.fit(X_train_up_vec, y_train_up)\n",
    "\n",
    "    y_pred_train = classifier.predict(X_train_up_vec)\n",
    "    acc = accuracy_score(y_train_up, y_pred_train)\n",
    "    train_accuracy.append(acc)\n",
    "\n",
    "    y_pred_test = classifier.predict(X_test_vec)\n",
    "    acc2 = accuracy_score(y_test, y_pred_test)\n",
    "    test_accuracy.append(acc2)\n",
    "\n",
    "data = {'max_features': max_list, 'train_accuracy': train_accuracy, 'test_accuracy': test_accuracy}\n",
    "\n",
    "max_df = pd.DataFrame(data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:17:59.748569300Z",
     "start_time": "2023-08-24T10:17:58.353443700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "   max_features  train_accuracy  test_accuracy\n0           300        0.970341       0.964429\n1           305        0.970341       0.964429\n2           310        0.970513       0.963928\n3           315        0.974713       0.971944\n4           320        0.975399       0.970441\n5           330        0.982342       0.976954\n6           340        0.986199       0.983968\n7           350        0.986628       0.983968",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>max_features</th>\n      <th>train_accuracy</th>\n      <th>test_accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>300</td>\n      <td>0.970341</td>\n      <td>0.964429</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>305</td>\n      <td>0.970341</td>\n      <td>0.964429</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>310</td>\n      <td>0.970513</td>\n      <td>0.963928</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>315</td>\n      <td>0.974713</td>\n      <td>0.971944</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>320</td>\n      <td>0.975399</td>\n      <td>0.970441</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>330</td>\n      <td>0.982342</td>\n      <td>0.976954</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>340</td>\n      <td>0.986199</td>\n      <td>0.983968</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>350</td>\n      <td>0.986628</td>\n      <td>0.983968</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:17:59.762537600Z",
     "start_time": "2023-08-24T10:17:59.749566700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing against original data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### The Model with 330 Max Features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression()",
      "text/html": "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                                 lowercase=True,\n",
    "                                 tokenizer = None,\n",
    "                                 preprocessor = None,\n",
    "                                 stop_words = None,\n",
    "                                 max_features = 330)\n",
    "\n",
    "X_train_up_vec = vectorizer.fit_transform(X_train_up)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train_up_vec, y_train_up)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:17:59.950030600Z",
     "start_time": "2023-08-24T10:17:59.792484400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### The New Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "half_test_df = pd.read_csv('raw_half_test.csv')\n",
    "heavy_test_df = pd.read_csv('raw_heavy_test.csv')\n",
    "light_test_df = pd.read_csv('raw_light_test.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:17:59.976957700Z",
     "start_time": "2023-08-24T10:17:59.955017800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Processing New Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from string import punctuation\n",
    "punctuation = list(punctuation)\n",
    "from nltk.stem import \tWordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def data_prep(data):\n",
    "\n",
    "    # tokenization\n",
    "    data['case_templ_token'] = data['case_templ'].apply(lambda x: tokenizer.tokenize(x))\n",
    "\n",
    "    # Removing Stopwords\n",
    "    def remove_stopwords(tokens):\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "        return filtered_tokens\n",
    "    data['case_templ_no_stopwords'] = data['case_templ_token'].apply(remove_stopwords)\n",
    "\n",
    "    # Removing Punctuation\n",
    "    def remove_punctuation(tokens):\n",
    "        clean_tokens = [token for token in tokens if token not in punctuation]\n",
    "        return clean_tokens\n",
    "    data['case_templ_clean'] = data['case_templ_no_stopwords'].apply(remove_punctuation)\n",
    "\n",
    "    # Lemmatize\n",
    "    def lemmatize_tokens(tokens):\n",
    "        lemmatized_tokens = [wordnet_lemmatizer.lemmatize(token) for token in tokens]\n",
    "        return lemmatized_tokens\n",
    "    data['lemmatized'] = data['case_templ_clean'].apply(lemmatize_tokens)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:18:00.307872500Z",
     "start_time": "2023-08-24T10:17:59.970974Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "data_prep(half_test_df)\n",
    "data_prep(heavy_test_df)\n",
    "data_prep(light_test_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:18:01.922024700Z",
     "start_time": "2023-08-24T10:18:00.307872500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "columns_to_drop = ['case_templ_token', 'case_templ_no_stopwords', 'case_templ_clean']\n",
    "half_test_df.drop(columns=columns_to_drop, inplace=True)\n",
    "heavy_test_df.drop(columns=columns_to_drop, inplace=True)\n",
    "light_test_df.drop(columns=columns_to_drop, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:18:01.937596900Z",
     "start_time": "2023-08-24T10:18:01.928536400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Combining X and y test so can add new datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "                                             lemmatized  target\n3205                     nothing worse cultural marxist       1\n9479                           troons complete 4ssholes       1\n4745     Putting coastal elite charge best thing anyone       0\n1315                           gangbangers burned stake       1\n3468                           Deathto cultural marxist       1\n...                                                 ...     ...\n1149                             dindus wonderful being       0\n3464  cultural marxist without doubt stupidestpeople...       1\n1152                            dindus blessing society       0\n2714                 Let's stand bings getting murdered       0\n5945                        going make banksterss smile       0\n\n[1996 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lemmatized</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3205</th>\n      <td>nothing worse cultural marxist</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9479</th>\n      <td>troons complete 4ssholes</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4745</th>\n      <td>Putting coastal elite charge best thing anyone</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1315</th>\n      <td>gangbangers burned stake</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3468</th>\n      <td>Deathto cultural marxist</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1149</th>\n      <td>dindus wonderful being</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3464</th>\n      <td>cultural marxist without doubt stupidestpeople...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1152</th>\n      <td>dindus blessing society</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2714</th>\n      <td>Let's stand bings getting murdered</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5945</th>\n      <td>going make banksterss smile</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1996 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xy_test = pd.concat([X_test, y_test], axis=1)\n",
    "Xy_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:18:01.980465500Z",
     "start_time": "2023-08-24T10:18:01.941571400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Creating Full New Test Datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "half_test = half_test_df[['lemmatized', 'target']].copy()\n",
    "half_test['lemmatized'] = [' '.join(x) for x in half_test['lemmatized']]\n",
    "half_test_combined = pd.concat([Xy_test, half_test])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:18:01.985452700Z",
     "start_time": "2023-08-24T10:18:01.976476600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "heavy_test = heavy_test_df[['lemmatized', 'target']].copy()\n",
    "heavy_test['lemmatized'] = [' '.join(x) for x in heavy_test['lemmatized']]\n",
    "heavy_test_combined = pd.concat([Xy_test, heavy_test])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:18:02.019361800Z",
     "start_time": "2023-08-24T10:18:01.989443400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "light_test = light_test_df[['lemmatized', 'target']].copy()\n",
    "light_test['lemmatized'] = [' '.join(x) for x in light_test['lemmatized']]\n",
    "light_test_combined = pd.concat([Xy_test, light_test])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:18:02.059291700Z",
     "start_time": "2023-08-24T10:18:02.004403700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Splitting into X and y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "half_test_combined_X = half_test_combined['lemmatized']\n",
    "half_test_combined_y = half_test_combined['target']\n",
    "half_test_X = half_test['lemmatized']\n",
    "half_test_y = half_test['target']\n",
    "heavy_test_combined_X = heavy_test_combined['lemmatized']\n",
    "heavy_test_combined_y = heavy_test_combined['target']\n",
    "heavy_test_X = heavy_test['lemmatized']\n",
    "heavy_test_y = heavy_test['target']\n",
    "light_test_combined_X = light_test_combined['lemmatized']\n",
    "light_test_combined_y = light_test_combined['target']\n",
    "light_test_X = light_test['lemmatized']\n",
    "light_test_y = light_test['target']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:18:02.122092300Z",
     "start_time": "2023-08-24T10:18:02.015372300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Vectorizing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "half_test_combined_X_vec = vectorizer.transform(half_test_combined_X)\n",
    "half_test_X_vec = vectorizer.transform((half_test_X))\n",
    "heavy_test_combined_X_vec = vectorizer.transform(heavy_test_combined_X)\n",
    "heavy_test_X_vec = vectorizer.transform((heavy_test_X))\n",
    "light_test_combined_X_vec = vectorizer.transform(light_test_combined_X)\n",
    "light_test_X_vec = vectorizer.transform((light_test_X))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:18:02.167001200Z",
     "start_time": "2023-08-24T10:18:02.067267300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Making Predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "half_test_combined_pred = classifier.predict(half_test_combined_X_vec)\n",
    "half_test_pred = classifier.predict(half_test_X_vec)\n",
    "heavy_test_combined_pred = classifier.predict(heavy_test_combined_X_vec)\n",
    "heavy_test_pred = classifier.predict(heavy_test_X_vec)\n",
    "light_test_combined_pred = classifier.predict(light_test_combined_X_vec)\n",
    "light_test_pred = classifier.predict(light_test_X_vec)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:18:02.167001200Z",
     "start_time": "2023-08-24T10:18:02.082194700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "half_test_combined_acc = accuracy_score(half_test_combined_y, half_test_combined_pred)\n",
    "half_test_acc = accuracy_score(half_test_y, half_test_pred)\n",
    "heavy_test_combined_acc = accuracy_score(heavy_test_combined_y, heavy_test_combined_pred)\n",
    "heavy_test_acc = accuracy_score(heavy_test_y, heavy_test_pred)\n",
    "light_test_combined_acc = accuracy_score(light_test_combined_y, light_test_combined_pred)\n",
    "light_test_acc = accuracy_score(light_test_y, light_test_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:18:02.167001200Z",
     "start_time": "2023-08-24T10:18:02.096158400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Accuracy Table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "              Datasets  Accuracy\n0   half_test_combined  0.976570\n1            half_test  0.900000\n2  heavy_test_combined  0.976570\n3           heavy_test  0.900000\n4  light_test_combined  0.977069\n5           light_test  1.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Datasets</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>half_test_combined</td>\n      <td>0.976570</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>half_test</td>\n      <td>0.900000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>heavy_test_combined</td>\n      <td>0.976570</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>heavy_test</td>\n      <td>0.900000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>light_test_combined</td>\n      <td>0.977069</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>light_test</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Datasets = ['half_test_combined', 'half_test', 'heavy_test_combined', 'heavy_test', 'light_test_combined', 'light_test']\n",
    "Accuracy = [half_test_combined_acc, half_test_acc, heavy_test_combined_acc, heavy_test_acc, light_test_combined_acc, light_test_acc]\n",
    "\n",
    "data = {'Datasets': Datasets, 'Accuracy': Accuracy}\n",
    "\n",
    "Acc_table = pd.DataFrame(data)\n",
    "Acc_table"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:18:02.167001200Z",
     "start_time": "2023-08-24T10:18:02.112114600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### In depth on 50/50"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "                                          case_templ  target  probability  \\\n0                      obama is another deadbeat dad       1     0.896984   \n1  another example of the dangerous impact of the...       1     0.788981   \n2            our streets are now overrun with dindus       1     0.889937   \n3  the demographic of my small town is now what y...       1     0.106196   \n4       this fits right into the cosmopolitan agenda       1     0.965240   \n5                     politicians are right scumbags       0     0.011448   \n6  ah, a cup of tea, the true great british pasttime       0     0.002157   \n7  I really hate what the council is doing to our...       0     0.105339   \n8  Modern media like netflix and hulu are so bori...       0     0.266182   \n9  people do not believe me when I say turkey is ...       0     0.000041   \n\n   prediction  confidence  \n0           1    2.164149  \n1           1    1.318793  \n2           1    2.090098  \n3           0   -2.130198  \n4           1    3.323899  \n5           0   -4.458452  \n6           0   -6.137067  \n7           0   -2.139260  \n8           0   -1.014078  \n9           0  -10.096707  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>case_templ</th>\n      <th>target</th>\n      <th>probability</th>\n      <th>prediction</th>\n      <th>confidence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>obama is another deadbeat dad</td>\n      <td>1</td>\n      <td>0.896984</td>\n      <td>1</td>\n      <td>2.164149</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>another example of the dangerous impact of the...</td>\n      <td>1</td>\n      <td>0.788981</td>\n      <td>1</td>\n      <td>1.318793</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>our streets are now overrun with dindus</td>\n      <td>1</td>\n      <td>0.889937</td>\n      <td>1</td>\n      <td>2.090098</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>the demographic of my small town is now what y...</td>\n      <td>1</td>\n      <td>0.106196</td>\n      <td>0</td>\n      <td>-2.130198</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>this fits right into the cosmopolitan agenda</td>\n      <td>1</td>\n      <td>0.965240</td>\n      <td>1</td>\n      <td>3.323899</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>politicians are right scumbags</td>\n      <td>0</td>\n      <td>0.011448</td>\n      <td>0</td>\n      <td>-4.458452</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>ah, a cup of tea, the true great british pasttime</td>\n      <td>0</td>\n      <td>0.002157</td>\n      <td>0</td>\n      <td>-6.137067</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>I really hate what the council is doing to our...</td>\n      <td>0</td>\n      <td>0.105339</td>\n      <td>0</td>\n      <td>-2.139260</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Modern media like netflix and hulu are so bori...</td>\n      <td>0</td>\n      <td>0.266182</td>\n      <td>0</td>\n      <td>-1.014078</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>people do not believe me when I say turkey is ...</td>\n      <td>0</td>\n      <td>0.000041</td>\n      <td>0</td>\n      <td>-10.096707</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "half_test_results = half_test_df.copy()\n",
    "half_test_results['probability'] = classifier.predict_proba(half_test_X_vec)[:,1]\n",
    "half_test_results['prediction'] = classifier.predict(half_test_X_vec)\n",
    "half_test_results['confidence'] = classifier.decision_function(half_test_X_vec)\n",
    "half_test_results.drop(columns='lemmatized', inplace=True)\n",
    "half_test_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:44:47.301159200Z",
     "start_time": "2023-08-24T10:44:47.255308800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "half_test_results.to_csv('LogRegLem.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:18:10.517821900Z",
     "start_time": "2023-08-24T10:18:10.485905900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### More Metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T12:46:11.073811800Z",
     "start_time": "2023-08-24T12:46:11.051997800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "def apr(y_pred, y_real):                                            # function to calculate the accuracy, precision and recall\n",
    "    \"\"\" Calculates accuracy, precision, recall\n",
    "        Requires predicted value first, and then the real value\n",
    "    \"\"\"\n",
    "    accuracy = metrics.accuracy_score(y_real, y_pred)\n",
    "    precision = metrics.precision_score(y_real, y_pred)\n",
    "    recall = metrics.recall_score(y_real, y_pred)\n",
    "    f1 = metrics.f1_score(y_real, y_pred)\n",
    "\n",
    "    print(f\"Accuracy:{accuracy}\")\n",
    "    print(f\"Precision:{precision}\")\n",
    "    print(f\"Recall:{recall}\")\n",
    "    print(f\"F1:{f1}\")\n",
    "    return accuracy, precision, recall, f1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T12:46:06.971637900Z",
     "start_time": "2023-08-24T12:46:06.947666800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9\n",
      "Precision:0.8\n",
      "Recall:1.0\n",
      "F1:0.888888888888889\n"
     ]
    },
    {
     "data": {
      "text/plain": "(0.9, 0.8, 1.0, 0.888888888888889)"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apr(half_test_y, half_test_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T12:46:12.877949500Z",
     "start_time": "2023-08-24T12:46:12.836280100Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
