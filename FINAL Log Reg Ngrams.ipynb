{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:12.442285500Z",
     "start_time": "2023-08-24T10:47:11.937601Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_lem.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:12.458258800Z",
     "start_time": "2023-08-24T10:47:11.958545100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "import ast ## This module can be used to evaluate literals, eg: transform string-lists back into lists\n",
    "def extract_genres(x):\n",
    "    x = ast.literal_eval(x)\n",
    "    return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:12.465191800Z",
     "start_time": "2023-08-24T10:47:11.988465Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "df['lemmatized'] = df['lemmatized'].apply(extract_genres)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:12.561932900Z",
     "start_time": "2023-08-24T10:47:12.004422800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "df['lemmatized'] = [' '.join(x) for x in df['lemmatized']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:12.564925100Z",
     "start_time": "2023-08-24T10:47:12.179954500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "X_a = df['lemmatized']\n",
    "\n",
    "y_a = df['target']\n",
    "\n",
    "X_train_a, X_test_a, y_train_a, y_test_a = train_test_split(X_a, y_a, test_size=0.2, random_state=14)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:12.565922100Z",
     "start_time": "2023-08-24T10:47:12.192919100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "                                             lemmatized  target\n1839                        nothing common ghetto trahs       1\n7092  never support politician treat autoandrophiles...       0\n9024              I'd rather cancre anywhere near Aiden       1\n5473                                   banker c n c e r       1\n9764                            Saying make bit bastard       0\n...                                                 ...     ...\n7526                               clownfishs beautiful       0\n6471                           hurt autogynephiles like       1\n2454                                   bings belong zoo       1\n9484                     obvious troons opposite stupid       0\n2667                           bings like absolutefilth       1\n\n[7983 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lemmatized</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1839</th>\n      <td>nothing common ghetto trahs</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7092</th>\n      <td>never support politician treat autoandrophiles...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9024</th>\n      <td>I'd rather cancre anywhere near Aiden</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5473</th>\n      <td>banker c n c e r</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9764</th>\n      <td>Saying make bit bastard</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7526</th>\n      <td>clownfishs beautiful</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6471</th>\n      <td>hurt autogynephiles like</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2454</th>\n      <td>bings belong zoo</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9484</th>\n      <td>obvious troons opposite stupid</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2667</th>\n      <td>bings like absolutefilth</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>7983 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xy_train_a = pd.concat([X_train_a, y_train_a], axis=1)\n",
    "Xy_train_a"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:12.580884400Z",
     "start_time": "2023-08-24T10:47:12.210872200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "1    5833\n0    2150\nName: target, dtype: int64"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xy_train_a.target.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:12.581881700Z",
     "start_time": "2023-08-24T10:47:12.227826600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "1    5833\n0    5833\nName: target, dtype: int64"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "majority = Xy_train_a[Xy_train_a['target'] == 1]\n",
    "minority = Xy_train_a[Xy_train_a['target'] == 0]\n",
    "minority_upsampled = resample(minority, replace=True, n_samples=5833, random_state=14)\n",
    "upsampled = pd.concat([majority, minority_upsampled])\n",
    "upsampled['target'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:12.596839Z",
     "start_time": "2023-08-24T10:47:12.243782200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "X_train_up_a = upsampled['lemmatized']\n",
    "y_train_up_a = upsampled['target']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:12.596839Z",
     "start_time": "2023-08-24T10:47:12.258743500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ngrams Main Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "# ngram_list = [(1,1), (1,2), (1,3), (2,2)]\n",
    "# train_accuracy = []\n",
    "# test_accuracy = []\n",
    "# for x, y in ngram_list:\n",
    "#     vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "#                                  lowercase=True,\n",
    "#                                  tokenizer = None,\n",
    "#                                  preprocessor = None,\n",
    "#                                  stop_words = None,\n",
    "#                                  ngram_range=(x, y),\n",
    "#                                  max_features = 340)\n",
    "#\n",
    "#     X_train_up_vec = vectorizer.fit_transform(X_train_up_a)\n",
    "#     X_test_vec = vectorizer.transform(X_test_a)\n",
    "#\n",
    "#     classifier = LogisticRegression(max_iter=1000)\n",
    "#     classifier.fit(X_train_up_vec, y_train_up_a)\n",
    "#\n",
    "#     y_pred_train = classifier.predict(X_train_up_vec)\n",
    "#     acc = accuracy_score(y_train_up_a, y_pred_train)\n",
    "#     train_accuracy.append(acc)\n",
    "#\n",
    "#     y_pred_test = classifier.predict(X_test_vec)\n",
    "#     acc2 = accuracy_score(y_test, y_pred_test)\n",
    "#     test_accuracy.append(acc2)\n",
    "#\n",
    "# data = {'max_features': ngram_list, 'train_accuracy': train_accuracy, 'test_accuracy': test_accuracy}\n",
    "#\n",
    "# ngram_df = pd.DataFrame(data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:12.597836800Z",
     "start_time": "2023-08-24T10:47:12.273705900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "# ngram_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:12.597836800Z",
     "start_time": "2023-08-24T10:47:12.289660Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Model with ngrams 1,2 and normal data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression(max_iter=1000)",
      "text/html": "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_a = CountVectorizer(analyzer = \"word\",\n",
    "                             lowercase=True,\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = None,\n",
    "                             max_features = 300,\n",
    "                             ngram_range=(1,2))\n",
    "\n",
    "X_train_up_vec_a = vectorizer_a.fit_transform(X_train_up_a)\n",
    "X_test_vec_a = vectorizer_a.transform(X_test_a)\n",
    "\n",
    "classifier_a = LogisticRegression(max_iter=1000)\n",
    "classifier_a.fit(X_train_up_vec_a, y_train_up_a)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:12.683607600Z",
     "start_time": "2023-08-24T10:47:12.365458700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ngrams With No Stopwords Removed Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "                                             case_templ       intent  \\\n0                                         I hate thugs.      hateful   \n1                  I really can't stand thugs like you.      hateful   \n2                                      I despise thugs.      hateful   \n3                                       I detest thugs.      hateful   \n4                            I absolutely loathe thugs.      hateful   \n...                                                 ...          ...   \n9974  Pro lifers should get the fuck out of our coun...  non-hateful   \n9975  Most gun dealers are seriously fucked in the h...  non-hateful   \n9976  By nature, corporate accountants are just shit...  non-hateful   \n9977     Most real estate agents are absolute bastards.  non-hateful   \n9978          Of course that dumbass was a snowboarder.  non-hateful   \n\n          category potential_dogwhistle  target  \\\n0           racist                  yes       1   \n1           racist                  yes       1   \n2           racist                  yes       1   \n3           racist                  yes       1   \n4           racist                  yes       1   \n...            ...                  ...     ...   \n9974  non-template                   no       0   \n9975  non-template                   no       0   \n9976  non-template                   no       0   \n9977  non-template                   no       0   \n9978  non-template                   no       0   \n\n                                       case_templ_token  \\\n0                           ['I', 'hate', 'thugs', '.']   \n1     ['I', 'really', \"can't\", 'stand', 'thugs', 'li...   \n2                        ['I', 'despise', 'thugs', '.']   \n3                         ['I', 'detest', 'thugs', '.']   \n4           ['I', 'absolutely', 'loathe', 'thugs', '.']   \n...                                                 ...   \n9974  ['Pro', 'lifers', 'should', 'get', 'the', 'fuc...   \n9975  ['Most', 'gun', 'dealers', 'are', 'seriously',...   \n9976  ['By', 'nature', ',', 'corporate', 'accountant...   \n9977  ['Most', 'real', 'estate', 'agents', 'are', 'a...   \n9978  ['Of', 'course', 'that', 'dumbass', 'was', 'a'...   \n\n                                case_templ_no_stopwords  \\\n0                                ['hate', 'thugs', '.']   \n1     ['really', \"can't\", 'stand', 'thugs', 'like', ...   \n2                             ['despise', 'thugs', '.']   \n3                              ['detest', 'thugs', '.']   \n4                ['absolutely', 'loathe', 'thugs', '.']   \n...                                                 ...   \n9974   ['Pro', 'lifers', 'get', 'fuck', 'country', '.']   \n9975  ['gun', 'dealers', 'seriously', 'fucked', 'hea...   \n9976  ['nature', ',', 'corporate', 'accountants', 's...   \n9977  ['real', 'estate', 'agents', 'absolute', 'bast...   \n9978          ['course', 'dumbass', 'snowboarder', '.']   \n\n                                       case_templ_clean  \\\n0                                ['I', 'hate', 'thugs']   \n1     ['I', 'really', \"can't\", 'stand', 'thugs', 'li...   \n2                             ['I', 'despise', 'thugs']   \n3                              ['I', 'detest', 'thugs']   \n4                ['I', 'absolutely', 'loathe', 'thugs']   \n...                                                 ...   \n9974  ['Pro', 'lifers', 'should', 'get', 'the', 'fuc...   \n9975  ['Most', 'gun', 'dealers', 'are', 'seriously',...   \n9976  ['By', 'nature', 'corporate', 'accountants', '...   \n9977  ['Most', 'real', 'estate', 'agents', 'are', 'a...   \n9978  ['Of', 'course', 'that', 'dumbass', 'was', 'a'...   \n\n                                             lemmatized  \n0                                 ['I', 'hate', 'thug']  \n1     ['I', 'really', \"can't\", 'stand', 'thug', 'lik...  \n2                              ['I', 'despise', 'thug']  \n3                               ['I', 'detest', 'thug']  \n4                 ['I', 'absolutely', 'loathe', 'thug']  \n...                                                 ...  \n9974  ['Pro', 'lifer', 'should', 'get', 'the', 'fuck...  \n9975  ['Most', 'gun', 'dealer', 'are', 'seriously', ...  \n9976  ['By', 'nature', 'corporate', 'accountant', 'a...  \n9977  ['Most', 'real', 'estate', 'agent', 'are', 'ab...  \n9978  ['Of', 'course', 'that', 'dumbass', 'wa', 'a',...  \n\n[9979 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>case_templ</th>\n      <th>intent</th>\n      <th>category</th>\n      <th>potential_dogwhistle</th>\n      <th>target</th>\n      <th>case_templ_token</th>\n      <th>case_templ_no_stopwords</th>\n      <th>case_templ_clean</th>\n      <th>lemmatized</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I hate thugs.</td>\n      <td>hateful</td>\n      <td>racist</td>\n      <td>yes</td>\n      <td>1</td>\n      <td>['I', 'hate', 'thugs', '.']</td>\n      <td>['hate', 'thugs', '.']</td>\n      <td>['I', 'hate', 'thugs']</td>\n      <td>['I', 'hate', 'thug']</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I really can't stand thugs like you.</td>\n      <td>hateful</td>\n      <td>racist</td>\n      <td>yes</td>\n      <td>1</td>\n      <td>['I', 'really', \"can't\", 'stand', 'thugs', 'li...</td>\n      <td>['really', \"can't\", 'stand', 'thugs', 'like', ...</td>\n      <td>['I', 'really', \"can't\", 'stand', 'thugs', 'li...</td>\n      <td>['I', 'really', \"can't\", 'stand', 'thug', 'lik...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I despise thugs.</td>\n      <td>hateful</td>\n      <td>racist</td>\n      <td>yes</td>\n      <td>1</td>\n      <td>['I', 'despise', 'thugs', '.']</td>\n      <td>['despise', 'thugs', '.']</td>\n      <td>['I', 'despise', 'thugs']</td>\n      <td>['I', 'despise', 'thug']</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I detest thugs.</td>\n      <td>hateful</td>\n      <td>racist</td>\n      <td>yes</td>\n      <td>1</td>\n      <td>['I', 'detest', 'thugs', '.']</td>\n      <td>['detest', 'thugs', '.']</td>\n      <td>['I', 'detest', 'thugs']</td>\n      <td>['I', 'detest', 'thug']</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I absolutely loathe thugs.</td>\n      <td>hateful</td>\n      <td>racist</td>\n      <td>yes</td>\n      <td>1</td>\n      <td>['I', 'absolutely', 'loathe', 'thugs', '.']</td>\n      <td>['absolutely', 'loathe', 'thugs', '.']</td>\n      <td>['I', 'absolutely', 'loathe', 'thugs']</td>\n      <td>['I', 'absolutely', 'loathe', 'thug']</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9974</th>\n      <td>Pro lifers should get the fuck out of our coun...</td>\n      <td>non-hateful</td>\n      <td>non-template</td>\n      <td>no</td>\n      <td>0</td>\n      <td>['Pro', 'lifers', 'should', 'get', 'the', 'fuc...</td>\n      <td>['Pro', 'lifers', 'get', 'fuck', 'country', '.']</td>\n      <td>['Pro', 'lifers', 'should', 'get', 'the', 'fuc...</td>\n      <td>['Pro', 'lifer', 'should', 'get', 'the', 'fuck...</td>\n    </tr>\n    <tr>\n      <th>9975</th>\n      <td>Most gun dealers are seriously fucked in the h...</td>\n      <td>non-hateful</td>\n      <td>non-template</td>\n      <td>no</td>\n      <td>0</td>\n      <td>['Most', 'gun', 'dealers', 'are', 'seriously',...</td>\n      <td>['gun', 'dealers', 'seriously', 'fucked', 'hea...</td>\n      <td>['Most', 'gun', 'dealers', 'are', 'seriously',...</td>\n      <td>['Most', 'gun', 'dealer', 'are', 'seriously', ...</td>\n    </tr>\n    <tr>\n      <th>9976</th>\n      <td>By nature, corporate accountants are just shit...</td>\n      <td>non-hateful</td>\n      <td>non-template</td>\n      <td>no</td>\n      <td>0</td>\n      <td>['By', 'nature', ',', 'corporate', 'accountant...</td>\n      <td>['nature', ',', 'corporate', 'accountants', 's...</td>\n      <td>['By', 'nature', 'corporate', 'accountants', '...</td>\n      <td>['By', 'nature', 'corporate', 'accountant', 'a...</td>\n    </tr>\n    <tr>\n      <th>9977</th>\n      <td>Most real estate agents are absolute bastards.</td>\n      <td>non-hateful</td>\n      <td>non-template</td>\n      <td>no</td>\n      <td>0</td>\n      <td>['Most', 'real', 'estate', 'agents', 'are', 'a...</td>\n      <td>['real', 'estate', 'agents', 'absolute', 'bast...</td>\n      <td>['Most', 'real', 'estate', 'agents', 'are', 'a...</td>\n      <td>['Most', 'real', 'estate', 'agent', 'are', 'ab...</td>\n    </tr>\n    <tr>\n      <th>9978</th>\n      <td>Of course that dumbass was a snowboarder.</td>\n      <td>non-hateful</td>\n      <td>non-template</td>\n      <td>no</td>\n      <td>0</td>\n      <td>['Of', 'course', 'that', 'dumbass', 'was', 'a'...</td>\n      <td>['course', 'dumbass', 'snowboarder', '.']</td>\n      <td>['Of', 'course', 'that', 'dumbass', 'was', 'a'...</td>\n      <td>['Of', 'course', 'that', 'dumbass', 'wa', 'a',...</td>\n    </tr>\n  </tbody>\n</table>\n<p>9979 rows × 9 columns</p>\n</div>"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = pd.read_csv('data_ngrams.csv')\n",
    "new"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:12.793316800Z",
     "start_time": "2023-08-24T10:47:12.608807300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "new['lemmatized'] = new['lemmatized'].apply(extract_genres)\n",
    "new['lemmatized'] = [' '.join(x) for x in new['lemmatized']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:13.016730600Z",
     "start_time": "2023-08-24T10:47:12.720510Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "X_b = new['lemmatized']\n",
    "\n",
    "y_b = new['target']\n",
    "\n",
    "X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(X_b, y_b, test_size=0.2, random_state=14)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:13.020716600Z",
     "start_time": "2023-08-24T10:47:13.006754500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "                                             lemmatized  target\n1839            You are nothing but common ghetto trahs       1\n7092  We should never support a politician that trea...       0\n9024  I'd rather have cancre than be anywhere near a...       1\n5473                           banker are a c a n c e r       1\n9764         Saying that doe make me a bit of a bastard       0\n...                                                 ...     ...\n7526                           clownfishs are beautiful       0\n6471  I will hurt you and any other autogynephiles l...       1\n2454                              bings belong in a zoo       1\n9484  It's obvious that troons are the very opposite...       0\n2667                   bings like you are absolutefilth       1\n\n[7983 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lemmatized</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1839</th>\n      <td>You are nothing but common ghetto trahs</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7092</th>\n      <td>We should never support a politician that trea...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9024</th>\n      <td>I'd rather have cancre than be anywhere near a...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5473</th>\n      <td>banker are a c a n c e r</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9764</th>\n      <td>Saying that doe make me a bit of a bastard</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7526</th>\n      <td>clownfishs are beautiful</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6471</th>\n      <td>I will hurt you and any other autogynephiles l...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2454</th>\n      <td>bings belong in a zoo</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9484</th>\n      <td>It's obvious that troons are the very opposite...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2667</th>\n      <td>bings like you are absolutefilth</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>7983 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xy_train_b = pd.concat([X_train_b, y_train_b], axis=1)\n",
    "Xy_train_b"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:13.035679700Z",
     "start_time": "2023-08-24T10:47:13.021716Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "1    5833\n0    5833\nName: target, dtype: int64"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "majority = Xy_train_b[Xy_train_b['target'] == 1]\n",
    "minority = Xy_train_b[Xy_train_b['target'] == 0]\n",
    "minority_upsampled = resample(minority, replace=True, n_samples=5833, random_state=14)\n",
    "upsampled = pd.concat([majority, minority_upsampled])\n",
    "upsampled['target'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:13.084546800Z",
     "start_time": "2023-08-24T10:47:13.038670900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "X_train_up_b = upsampled['lemmatized']\n",
    "y_train_up_b = upsampled['target']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:13.110478400Z",
     "start_time": "2023-08-24T10:47:13.052634400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "# ngram_list = [(1,1), (1,2), (1,3), (2,2)]\n",
    "# train_accuracy = []\n",
    "# test_accuracy = []\n",
    "# for x, y in ngram_list:\n",
    "#     vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "#                                  lowercase=True,\n",
    "#                                  tokenizer = None,\n",
    "#                                  preprocessor = None,\n",
    "#                                  stop_words = None,\n",
    "#                                  ngram_range=(x, y),\n",
    "#                                  max_features = 340)\n",
    "#\n",
    "#     X_train_up_vec = vectorizer.fit_transform(X_train_up_b)\n",
    "#     X_test_vec = vectorizer.transform(X_test_b)\n",
    "#\n",
    "#     classifier = LogisticRegression(max_iter=1000)\n",
    "#     classifier.fit(X_train_up_vec, y_train_up_b)\n",
    "#\n",
    "#     y_pred_train = classifier.predict(X_train_up_vec)\n",
    "#     acc = accuracy_score(y_train_up_b, y_pred_train)\n",
    "#     train_accuracy.append(acc)\n",
    "#\n",
    "#     y_pred_test = classifier.predict(X_test_vec)\n",
    "#     acc2 = accuracy_score(y_test, y_pred_test)\n",
    "#     test_accuracy.append(acc2)\n",
    "#\n",
    "# data = {'max_features': ngram_list, 'train_accuracy': train_accuracy, 'test_accuracy': test_accuracy}\n",
    "#\n",
    "# ngram_df = pd.DataFrame(data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:13.110478400Z",
     "start_time": "2023-08-24T10:47:13.068590700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "# ngram_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:13.138401600Z",
     "start_time": "2023-08-24T10:47:13.083560200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression(max_iter=1000)",
      "text/html": "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_b = CountVectorizer(analyzer = \"word\",\n",
    "                             lowercase=True,\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = None,\n",
    "                             max_features = 300,\n",
    "                             ngram_range=(1,2))\n",
    "\n",
    "X_train_up_vec_b = vectorizer_b.fit_transform(X_train_up_b)\n",
    "X_test_vec_b = vectorizer_b.transform(X_test_b)\n",
    "\n",
    "classifier_b = LogisticRegression(max_iter=1000)\n",
    "classifier_b.fit(X_train_up_vec_b, y_train_up_b)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:13.485239900Z",
     "start_time": "2023-08-24T10:47:13.126435700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing Both Against New Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Creating Brand New Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "sentences_dogwhistles = ['our streets are now overrun with dindus', 'hate to see these welfare queens with their new iphones', 'if we are not careful they will turn your child into a clownfish', 'obama is another deadbeat dad', 'this fits right into the cosmopolitan agenda', 'the demographic of my small town is now what you might call urban', 'these cultural marxists will have us mating with thugs', 'imagine finding out your child is a durden', 'we have to take down the coastal elite', 'another example of the dangerous impact of the genderist agenda on our children']\n",
    "\n",
    "sentences_not_dogwhistles = ['I really hate what the council is doing to our streets', 'I love indian food, give me all the spice you have', 'why do the crows look at me so funny?', 'politicians are right scumbags', 'people do not believe me when I say turkey is a great holiday destination', 'Modern media like netflix and hulu are so boring and melting our brains', 'we all have a responsibility to preserve our town centers in the best condition possible', 'hated what man u did last week, dreadful defending', 'the weatehr is always so shit in this country', 'ah, a cup of tea, the true great british pasttime']\n",
    "\n",
    "print(len(sentences_dogwhistles))\n",
    "print(len(sentences_not_dogwhistles))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:13.534109100Z",
     "start_time": "2023-08-24T10:47:13.488231900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "data": {
      "text/plain": "                                          case_templ  target\n0            our streets are now overrun with dindus       1\n1  hate to see these welfare queens with their ne...       1\n2  if we are not careful they will turn your chil...       1\n3                      obama is another deadbeat dad       1\n4       this fits right into the cosmopolitan agenda       1\n5  the demographic of my small town is now what y...       1\n6  these cultural marxists will have us mating wi...       1\n7         imagine finding out your child is a durden       1\n8             we have to take down the coastal elite       1\n9  another example of the dangerous impact of the...       1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>case_templ</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>our streets are now overrun with dindus</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>hate to see these welfare queens with their ne...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>if we are not careful they will turn your chil...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>obama is another deadbeat dad</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>this fits right into the cosmopolitan agenda</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>the demographic of my small town is now what y...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>these cultural marxists will have us mating wi...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>imagine finding out your child is a durden</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>we have to take down the coastal elite</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>another example of the dangerous impact of the...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dw_df = pd.DataFrame(sentences_dogwhistles)\n",
    "test_dw_df.rename(columns={0:'case_templ'}, inplace=True)\n",
    "test_dw_df['target'] = 1\n",
    "test_dw_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:13.571009700Z",
     "start_time": "2023-08-24T10:47:13.503190800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "data": {
      "text/plain": "                                          case_templ  target\n0  I really hate what the council is doing to our...       0\n1  I love indian food, give me all the spice you ...       0\n2              why do the crows look at me so funny?       0\n3                     politicians are right scumbags       0\n4  people do not believe me when I say turkey is ...       0\n5  Modern media like netflix and hulu are so bori...       0\n6  we all have a responsibility to preserve our t...       0\n7  hated what man u did last week, dreadful defen...       0\n8      the weatehr is always so shit in this country       0\n9  ah, a cup of tea, the true great british pasttime       0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>case_templ</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I really hate what the council is doing to our...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I love indian food, give me all the spice you ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>why do the crows look at me so funny?</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>politicians are right scumbags</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>people do not believe me when I say turkey is ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Modern media like netflix and hulu are so bori...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>we all have a responsibility to preserve our t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>hated what man u did last week, dreadful defen...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>the weatehr is always so shit in this country</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>ah, a cup of tea, the true great british pasttime</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ndw_df = pd.DataFrame(sentences_not_dogwhistles)\n",
    "test_ndw_df.rename(columns={0:'case_templ'}, inplace=True)\n",
    "test_ndw_df['target'] = 0\n",
    "test_ndw_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:13.572007400Z",
     "start_time": "2023-08-24T10:47:13.519149100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Processing Original"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from string import punctuation\n",
    "punctuation = list(punctuation)\n",
    "from nltk.stem import \tWordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def data_prep(data):\n",
    "\n",
    "    # tokenization\n",
    "    data['case_templ_token'] = data['case_templ'].apply(lambda x: tokenizer.tokenize(x))\n",
    "\n",
    "    # Removing Stopwords\n",
    "    def remove_stopwords(tokens):\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "        return filtered_tokens\n",
    "    data['case_templ_no_stopwords'] = data['case_templ_token'].apply(remove_stopwords)\n",
    "\n",
    "    # Removing Punctuation\n",
    "    def remove_punctuation(tokens):\n",
    "        clean_tokens = [token for token in tokens if token not in punctuation]\n",
    "        return clean_tokens\n",
    "    data['case_templ_clean'] = data['case_templ_no_stopwords'].apply(remove_punctuation)\n",
    "\n",
    "    # Lemmatize\n",
    "    def lemmatize_tokens(tokens):\n",
    "        lemmatized_tokens = [wordnet_lemmatizer.lemmatize(token) for token in tokens]\n",
    "        return lemmatized_tokens\n",
    "    data['lemmatized'] = data['case_templ_clean'].apply(lemmatize_tokens)\n",
    "\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:13.613897600Z",
     "start_time": "2023-08-24T10:47:13.559198700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "a_test_dw_df = data_prep(test_dw_df)\n",
    "a_test_dw_df_full = a_test_dw_df[['case_templ', 'lemmatized', 'target']]\n",
    "a_test_dw_df = a_test_dw_df[['lemmatized', 'target']]\n",
    "a_test_ndw_df = data_prep(test_ndw_df)\n",
    "a_test_ndw_df_full = a_test_ndw_df[['case_templ', 'lemmatized', 'target']]\n",
    "a_test_ndw_df = a_test_ndw_df[['lemmatized', 'target']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:13.769479400Z",
     "start_time": "2023-08-24T10:47:13.566023Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "data": {
      "text/plain": "                                          lemmatized  target\n3                         obama another deadbeat dad       1\n9  another example dangerous impact genderist age...       1\n0                              street overrun dindus       1\n5            demographic small town might call urban       1\n4                      fit right cosmopolitan agenda       1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lemmatized</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>obama another deadbeat dad</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>another example dangerous impact genderist age...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>street overrun dindus</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>demographic small town might call urban</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>fit right cosmopolitan agenda</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_half_test_dw_df = a_test_dw_df.sample(n=5, random_state=14)\n",
    "a_half_test_ndw_df = a_test_ndw_df.sample(n=5, random_state=14)\n",
    "a_half_test = pd.concat([a_half_test_dw_df, a_half_test_ndw_df])\n",
    "a_half_test['lemmatized'] = [' '.join(x) for x in a_half_test['lemmatized']]\n",
    "a_half_test.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:13.781480200Z",
     "start_time": "2023-08-24T10:47:13.643815500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "a_Xy_test = pd.concat([X_test_a, y_test_a], axis=1)\n",
    "a_Xy_test_new = pd.concat([a_Xy_test, a_half_test])\n",
    "a_new_X_test = a_Xy_test_new['lemmatized']\n",
    "a_new_y_test = a_Xy_test_new['target']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:13.787430800Z",
     "start_time": "2023-08-24T10:47:13.657780Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Processing in Ngrams Style:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from string import punctuation\n",
    "punctuation = list(punctuation)\n",
    "from nltk.stem import \tWordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def ngram_prep(data):\n",
    "\n",
    "    # tokenization\n",
    "    data['case_templ_token'] = data['case_templ'].apply(lambda x: tokenizer.tokenize(x))\n",
    "\n",
    "    # Removing Punctuation\n",
    "    def remove_punctuation(tokens):\n",
    "        clean_tokens = [token for token in tokens if token not in punctuation]\n",
    "        return clean_tokens\n",
    "    data['case_templ_clean'] = data['case_templ_token'].apply(remove_punctuation)\n",
    "\n",
    "    # Lemmatize\n",
    "    def lemmatize_tokens(tokens):\n",
    "        lemmatized_tokens = [wordnet_lemmatizer.lemmatize(token) for token in tokens]\n",
    "        return lemmatized_tokens\n",
    "    data['lemmatized'] = data['case_templ_clean'].apply(lemmatize_tokens)\n",
    "\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:13.788429800Z",
     "start_time": "2023-08-24T10:47:13.677724Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "b_test_dw_df = ngram_prep(test_dw_df)\n",
    "b_test_dw_df_full = b_test_dw_df[['case_templ', 'lemmatized', 'target']]\n",
    "b_test_dw_df = b_test_dw_df[['lemmatized', 'target']]\n",
    "b_test_ndw_df = ngram_prep(test_ndw_df)\n",
    "b_test_ndw_df_full = b_test_ndw_df[['case_templ', 'lemmatized', 'target']]\n",
    "b_test_ndw_df = b_test_ndw_df[['lemmatized', 'target']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:13.788429800Z",
     "start_time": "2023-08-24T10:47:13.707670Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "                                          lemmatized  target\n3                      obama is another deadbeat dad       1\n9  another example of the dangerous impact of the...       1\n0             our street are now overrun with dindus       1\n5  the demographic of my small town is now what y...       1\n4        this fit right into the cosmopolitan agenda       1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lemmatized</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>obama is another deadbeat dad</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>another example of the dangerous impact of the...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>our street are now overrun with dindus</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>the demographic of my small town is now what y...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>this fit right into the cosmopolitan agenda</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_half_test_dw_df = b_test_dw_df.sample(n=5, random_state=14)\n",
    "b_half_test_ndw_df = b_test_ndw_df.sample(n=5, random_state=14)\n",
    "b_half_test = pd.concat([b_half_test_dw_df, b_half_test_ndw_df])\n",
    "b_half_test['lemmatized'] = [' '.join(x) for x in b_half_test['lemmatized']]\n",
    "b_half_test.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:13.789425500Z",
     "start_time": "2023-08-24T10:47:13.727590Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "b_Xy_test = pd.concat([X_test_b, y_test_b], axis=1)\n",
    "b_Xy_test_new = pd.concat([b_Xy_test, b_half_test])\n",
    "b_new_X_test = b_Xy_test_new['lemmatized']\n",
    "b_new_y_test = b_Xy_test_new['target']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:13.790422500Z",
     "start_time": "2023-08-24T10:47:13.737564700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Performance Comparison"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### apr scores function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def apr(y_pred, y_real):  # function to calculate the accuracy, precision and recall\n",
    "    \"\"\" Calculates accuracy, precision, recall\n",
    "        Requires predicted value first, and then the real value\n",
    "    \"\"\"\n",
    "    accuracy = metrics.accuracy_score(y_real, y_pred)\n",
    "    precision = metrics.precision_score(y_real, y_pred)\n",
    "    recall = metrics.recall_score(y_real, y_pred)\n",
    "    f1 = metrics.f1_score(y_real, y_pred)\n",
    "\n",
    "    print(f\"Accuracy:{accuracy}\")\n",
    "    print(f\"Precision:{precision}\")\n",
    "    print(f\"Recall:{recall}\")\n",
    "    print(f\"F1:{f1}\")\n",
    "    return accuracy, precision, recall, f1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:13.790422500Z",
     "start_time": "2023-08-24T10:47:13.752524900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### a predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "a_new_X_test_vec = vectorizer_a.transform(a_new_X_test)\n",
    "a_new_y_pred_test = classifier_a.predict(a_new_X_test_vec)\n",
    "a_new_acc = accuracy_score(a_new_y_test, a_new_y_pred_test)\n",
    "a_nnew_X_test = a_half_test['lemmatized']\n",
    "a_nnew_y_test = a_half_test['target']\n",
    "a_nnew_X_test_vec = vectorizer_a.transform(a_nnew_X_test)\n",
    "a_nnew_y_pred_test = classifier_a.predict(a_nnew_X_test_vec)\n",
    "a_nnew_acc = accuracy_score(a_nnew_y_test, a_nnew_y_pred_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:13.853256Z",
     "start_time": "2023-08-24T10:47:13.790422500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### b predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "b_new_X_test_vec = vectorizer_b.transform(b_new_X_test)\n",
    "b_new_y_pred_test = classifier_b.predict(b_new_X_test_vec)\n",
    "b_new_acc = accuracy_score(b_new_y_test, b_new_y_pred_test)\n",
    "b_nnew_X_test = b_half_test['lemmatized']\n",
    "b_nnew_y_test = b_half_test['target']\n",
    "b_nnew_X_test_vec = vectorizer_b.transform(b_nnew_X_test)\n",
    "b_nnew_y_pred_test = classifier_b.predict(b_nnew_X_test_vec)\n",
    "b_nnew_acc = accuracy_score(b_nnew_y_test, b_nnew_y_pred_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:13.934038500Z",
     "start_time": "2023-08-24T10:47:13.798403800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Summary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of large a test set is:   0.8998005982053838\n",
      "Accuracy of small a test set is:   0.9\n",
      "Accuracy of large b test set is:   0.9127617148554337\n",
      "Accuracy of small b test set is:   0.7\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of large a test set is:', ' ', a_new_acc)\n",
    "print('Accuracy of small a test set is:', ' ', a_nnew_acc)\n",
    "print('Accuracy of large b test set is:', ' ', b_new_acc)\n",
    "print('Accuracy of small b test set is:', ' ', b_nnew_acc)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:13.967981500Z",
     "start_time": "2023-08-24T10:47:13.845277200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Sentence by Sentence breakdown"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "                                          case_templ  target\n3                      obama is another deadbeat dad       1\n9  another example of the dangerous impact of the...       1\n0            our streets are now overrun with dindus       1\n5  the demographic of my small town is now what y...       1\n4       this fits right into the cosmopolitan agenda       1\n3                     politicians are right scumbags       0\n9  ah, a cup of tea, the true great british pasttime       0\n0  I really hate what the council is doing to our...       0\n5  Modern media like netflix and hulu are so bori...       0\n4  people do not believe me when I say turkey is ...       0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>case_templ</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>obama is another deadbeat dad</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>another example of the dangerous impact of the...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>our streets are now overrun with dindus</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>the demographic of my small town is now what y...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>this fits right into the cosmopolitan agenda</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>politicians are right scumbags</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>ah, a cup of tea, the true great british pasttime</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>I really hate what the council is doing to our...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Modern media like netflix and hulu are so bori...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>people do not believe me when I say turkey is ...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "half_test_dw_df_full = a_test_dw_df_full.sample(n=5, random_state=14)\n",
    "half_test_ndw_df_full = a_test_ndw_df_full.sample(n=5, random_state=14)\n",
    "half_test_full = pd.concat([half_test_dw_df_full, half_test_ndw_df_full])\n",
    "half_test_full.drop(columns='lemmatized', inplace=True)\n",
    "half_test_full"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:14.042775400Z",
     "start_time": "2023-08-24T10:47:13.867218500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "data": {
      "text/plain": "                                          case_templ  target  probability_a  \\\n3                      obama is another deadbeat dad       1       0.658373   \n9  another example of the dangerous impact of the...       1       0.503509   \n0            our streets are now overrun with dindus       1       0.650972   \n5  the demographic of my small town is now what y...       1       0.005071   \n4       this fits right into the cosmopolitan agenda       1       0.754338   \n3                     politicians are right scumbags       0       0.006082   \n9  ah, a cup of tea, the true great british pasttime       0       0.058045   \n0  I really hate what the council is doing to our...       0       0.048381   \n5  Modern media like netflix and hulu are so bori...       0       0.045789   \n4  people do not believe me when I say turkey is ...       0       0.029317   \n\n   prediction_a  probability_b  prediction_b  confidence  \n3             1       0.256187             0   -1.065882  \n9             1       0.314979             0   -0.776942  \n0             1       0.901958             1    2.219168  \n5             0       0.000532             0   -7.538271  \n4             1       0.861105             1    1.824495  \n3             0       0.126864             0   -1.928973  \n9             0       0.031259             0   -3.433693  \n0             0       0.245147             0   -1.124664  \n5             0       0.002197             0   -6.118538  \n4             0       0.000470             0   -7.662238  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>case_templ</th>\n      <th>target</th>\n      <th>probability_a</th>\n      <th>prediction_a</th>\n      <th>probability_b</th>\n      <th>prediction_b</th>\n      <th>confidence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>obama is another deadbeat dad</td>\n      <td>1</td>\n      <td>0.658373</td>\n      <td>1</td>\n      <td>0.256187</td>\n      <td>0</td>\n      <td>-1.065882</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>another example of the dangerous impact of the...</td>\n      <td>1</td>\n      <td>0.503509</td>\n      <td>1</td>\n      <td>0.314979</td>\n      <td>0</td>\n      <td>-0.776942</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>our streets are now overrun with dindus</td>\n      <td>1</td>\n      <td>0.650972</td>\n      <td>1</td>\n      <td>0.901958</td>\n      <td>1</td>\n      <td>2.219168</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>the demographic of my small town is now what y...</td>\n      <td>1</td>\n      <td>0.005071</td>\n      <td>0</td>\n      <td>0.000532</td>\n      <td>0</td>\n      <td>-7.538271</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>this fits right into the cosmopolitan agenda</td>\n      <td>1</td>\n      <td>0.754338</td>\n      <td>1</td>\n      <td>0.861105</td>\n      <td>1</td>\n      <td>1.824495</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>politicians are right scumbags</td>\n      <td>0</td>\n      <td>0.006082</td>\n      <td>0</td>\n      <td>0.126864</td>\n      <td>0</td>\n      <td>-1.928973</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>ah, a cup of tea, the true great british pasttime</td>\n      <td>0</td>\n      <td>0.058045</td>\n      <td>0</td>\n      <td>0.031259</td>\n      <td>0</td>\n      <td>-3.433693</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>I really hate what the council is doing to our...</td>\n      <td>0</td>\n      <td>0.048381</td>\n      <td>0</td>\n      <td>0.245147</td>\n      <td>0</td>\n      <td>-1.124664</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Modern media like netflix and hulu are so bori...</td>\n      <td>0</td>\n      <td>0.045789</td>\n      <td>0</td>\n      <td>0.002197</td>\n      <td>0</td>\n      <td>-6.118538</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>people do not believe me when I say turkey is ...</td>\n      <td>0</td>\n      <td>0.029317</td>\n      <td>0</td>\n      <td>0.000470</td>\n      <td>0</td>\n      <td>-7.662238</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "half_test_results = half_test_full.copy()\n",
    "half_test_results['probability_a'] = classifier_a.predict_proba(a_nnew_X_test_vec)[:,1]\n",
    "half_test_results['prediction_a'] = classifier_a.predict(a_nnew_X_test_vec)\n",
    "half_test_results['probability_b'] = classifier_b.predict_proba(b_nnew_X_test_vec)[:,1]\n",
    "half_test_results['prediction_b'] = classifier_b.predict(b_nnew_X_test_vec)\n",
    "half_test_results['confidence'] = classifier_b.decision_function(b_nnew_X_test_vec)\n",
    "half_test_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T10:47:14.070706500Z",
     "start_time": "2023-08-24T10:47:13.892151Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "half_test_results.to_csv('LogReg_ngrams.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T12:20:01.219977400Z",
     "start_time": "2023-08-24T12:20:01.078356300Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
